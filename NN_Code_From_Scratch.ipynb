{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'DESCR', 'target_names', 'feature_names', 'target'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = data['data'],columns = data['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(df)\n",
    "y = np.array(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96078431  0.92156863  0.9375    ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X,y)\n",
    "print(cross_val_score(clf, X, y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "enc = OneHotEncoder()\n",
    "y_train = enc.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1 = np.random.randn(4,10)*.001\n",
    "b1 = np.random.randn(10)*.001\n",
    "w2 = np.random.randn(10,10)*.001\n",
    "b2 = np.random.randn(10)*.001\n",
    "w_out = np.random.randn(10,3)*.001\n",
    "b_out = np.random.randn(3)*.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Stochastic Gradient Descent\n",
    "(1x4)(4X10)+(1x10) = (1x10) \n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(x,y,w1,b1,w2,b2,w_out,b_out):\n",
    "    '''\n",
    "    print('SHAPES GOING INTO FORWARD-PASS')\n",
    "    print('w1: ',w1.shape)\n",
    "    print('w2: ',w2.shape)\n",
    "    print('b1: ',b1.shape)\n",
    "    print('b2: ',b2.shape)\n",
    "    print('w_out: ',w_out.shape)\n",
    "    '''\n",
    "    #forward pass for x (SGD)\n",
    "    fc1 = (x.dot(w1)+b1).reshape(10,1)\n",
    "    #(1x4)(4X10)+(1x10) = (1x10)\n",
    "    activation_1 = sigmoid(fc1).T\n",
    "    #(1x10)\n",
    "    \n",
    "    fc2 = (activation_1.dot(w2)+b2).reshape(10,1)\n",
    "    #(1x10)(10x10)=(1x10)\n",
    "    activation_2 = sigmoid(fc2).T\n",
    "    #(1x10)\n",
    "    \n",
    "    output = (activation_2.dot(w_out)+b_out).reshape(3,1)\n",
    "    #(1x10)(10x3)=(1x3)\n",
    "    class_probs = softmax(output)\n",
    "    #(1x4)\n",
    "    cost = MSE(class_probs,y)\n",
    "    '''\n",
    "    print('SHAPES COMING OUT OF FORWARD-PASS')\n",
    "    print('w1: ',w1.shape)\n",
    "    print('w2: ',w2.shape)\n",
    "    print('b1: ',b1.shape)\n",
    "    print('b2: ',b2.shape)\n",
    "    print('w_out: ',w_out.shape)\n",
    "    print('fc1: ', fc1.shape)\n",
    "    print('activation_1: ', activation_1.shape)\n",
    "    print('fc2: ', fc2.shape)\n",
    "    print('activation_2: ',activation_2.shape)\n",
    "    print('output: ',output.shape)\n",
    "    print('class_probs: ', class_probs.shape)\n",
    "    '''\n",
    "    return fc1, activation_1, fc2, activation_2, output, class_probs, cost\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# def softmax(z):\n",
    "#     assert len(z.shape) == 2\n",
    "#     s = np.max(z, axis=1)\n",
    "#     s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "#     e_x = np.exp(z - s)\n",
    "#     div = np.sum(e_x, axis=0)\n",
    "#     div = div[:, np.newaxis] # dito\n",
    "#     return e_x / div\n",
    "\n",
    "def softmax(w, t = 1.0):\n",
    "    e = np.exp(np.array(w) / t)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist\n",
    "\n",
    "def MSE(class_probs,y):\n",
    "    #using a simple SSE cost function for ease of differentiation.\n",
    "    return np.sum((class_probs.T-y)**2)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2082902 ],\n",
       "       [ 0.76948111],\n",
       "       [ 0.02222869]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z= np.array([[-4.45923876],\n",
    "             [-3.15245467],\n",
    "             [-6.69678721]])\n",
    "# z= np.array([[0.45923876],\n",
    "#              [1.15245467],\n",
    "#              [0.69678721]])\n",
    "softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = X[0]\n",
    "y = y_train[0]\n",
    "fc1, activation_1, fc2, activation_2,output,class_probs, cost = forward(x,y,w1,b1,w2,b2,w_out,b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "x > fc1(w1,b1) > activation_1(sigmoid) > \n",
    "fc2(w2,b2) > activation_2(sigmoid) >\n",
    "output(w_out) > class_probs(softmax) > cost(MSE)\n",
    "\n",
    "cost(class_probs) = np.sum((class_probs.T-y)**2)/3\n",
    "                    2*(class_probs[0]+class_probs[1]+class_probs[2])\n",
    "d_cost/d_class_probs = np.array([2*class_probs[0],2*class_probs[1],2*class_probs[2]]) \n",
    "\n",
    "d_class_probs_d_output = d_softmax/d_output\n",
    "(1x3) > (1x3)\n",
    "'''\n",
    "\n",
    "def d_MSE(class_probs):\n",
    "    #IN: cost(1)\n",
    "    #OUT: dcost/dclass_probs(1x3)\n",
    "    return np.array([class_probs[0],class_probs[1],class_probs[2]]).T*2\n",
    "    \n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def d_softmax(class_probs):\n",
    "    return softmax(class_probs)*(1-softmax(class_probs))\n",
    "\n",
    "def backward(w1,w2,b1,b2,w_out,b_out,fc1, activation_1, fc2, activation_2,output,class_probs,cost,learning_rate):\n",
    "    '''\n",
    "    print('SHAPES GOING INTO BACKPASS')\n",
    "    print('w1: ',w1.shape)\n",
    "    print('w2: ',w2.shape)\n",
    "    print('b1: ',b1.shape)\n",
    "    print('b2: ',b2.shape)\n",
    "    print('w_out: ',w_out.shape)\n",
    "    print('fc1: ', fc1.shape)\n",
    "    print('activation_1: ', activation_1.shape)\n",
    "    print('fc2: ', fc2.shape)\n",
    "    print('activation_2: ',activation_2.shape)\n",
    "    print('output: ',output.shape)\n",
    "    print('class_probs: ', class_probs.shape)\n",
    "    '''\n",
    "    d_cost_d_class_probs = d_MSE(class_probs).T\n",
    "    #print('d_cost_d_class_probs',d_cost_d_class_probs.shape)\n",
    "    d_class_probs_d_output = d_softmax(output)\n",
    "    #print('d_class_probs_d_output',d_class_probs_d_output.shape)\n",
    "    d_output_d_activation_2 = w_out\n",
    "    #print('d_output_d_activation_2',d_output_d_activation_2.shape)\n",
    "    d_activation_2_d_fc2 = d_sigmoid(fc2)\n",
    "    #print('d_activation_2_d_fc2',d_activation_2_d_fc2.shape)\n",
    "    d_fc2_d_activation1 = w2\n",
    "    #print('d_fc2_d_activation1',d_fc2_d_activation1.shape)\n",
    "    d_activation_1_d_fc1 = d_sigmoid(fc1)\n",
    "    #print('d_activation_1_d_fc1',d_activation_1_d_fc1.shape)\n",
    "    d_fc1_d_x = w1\n",
    "    #print('d_fc1_d_x',d_fc1_d_x.shape, '\\n \\n')\n",
    "    \n",
    "    d_cost_d_output = d_cost_d_class_probs * d_class_probs_d_output\n",
    "    #print('d_cost_d_output',d_cost_d_output.shape, '\\n \\n')\n",
    "    \n",
    "    d_cost_d_w_out = (d_cost_d_output * activation_2).T\n",
    "    #print('d_cost_d_w_out',d_cost_d_w_out.shape, '\\n \\n')\n",
    "    d_cost_d_b_out = d_cost_d_output\n",
    "    #print('d_cost_d_b_out',d_cost_d_b_out.shape, '\\n \\n')\n",
    "    \n",
    "    d_cost_d_activation_2 = d_cost_d_output.T.dot(d_output_d_activation_2.T)\n",
    "    #print('d_cost_d_activation_2',d_cost_d_activation_2.shape)\n",
    "    \n",
    "    d_cost_d_fc2 = d_cost_d_activation_2 * d_activation_2_d_fc2\n",
    "    #print('d_cost_d_fc2',d_cost_d_fc2.shape, '\\n \\n')\n",
    "    \n",
    "    d_cost_d_w2 = d_cost_d_activation_2.T.dot(fc2.reshape(1,10))\n",
    "    #print('d_cost_d_w2',d_cost_d_w2.shape)\n",
    "    d_cost_d_b2 = d_cost_d_activation_2\n",
    "    #print('d_cost_d_b2',d_cost_d_b2.shape, '\\n \\n')\n",
    "    \n",
    "    d_cost_d_activation_1 = d_cost_d_fc2 * d_fc2_d_activation1\n",
    "    #print('d_cost_d_activation_1',d_cost_d_activation_1.shape)\n",
    "    \n",
    "    d_cost_d_fc1 = d_cost_d_activation_1 * d_activation_1_d_fc1\n",
    "    #print('d_cost_d_fc1',d_cost_d_fc1.shape, '\\n \\n')\n",
    "    \n",
    "    d_cost_d_w1 = d_cost_d_activation_1.T.dot(fc1.reshape(10,1))\n",
    "    #print('d_cost_d_w1',d_cost_d_w1.shape)\n",
    "    d_cost_d_b1 = d_cost_d_activation_2\n",
    "    #print('d_cost_d_b1',d_cost_d_b1.shape, '\\n \\n')\n",
    "    \n",
    "    d_cost_d_x = d_cost_d_fc1.dot(d_fc1_d_x.T)\n",
    "    #print('d_cost_d_x',d_cost_d_x.shape)\n",
    "    \n",
    "    w_out = w_out-learning_rate*d_cost_d_w_out\n",
    "    b_out = b_out-learning_rate*d_cost_d_b_out\n",
    "    \n",
    "    w2 = w2-learning_rate*d_cost_d_w2.T\n",
    "    b2 = b2-learning_rate*d_cost_d_b2\n",
    "\n",
    "    w1 = w1-learning_rate*d_cost_d_w1.T\n",
    "    b1 = b1-learning_rate*d_cost_d_b1\n",
    "    '''\n",
    "    print('SHAPES COMING OUT OF BACKPASS')\n",
    "    print('w1: ',w1.shape)\n",
    "    print('w2: ',w2.shape)\n",
    "    print('b1: ',b1.shape)\n",
    "    print('b2: ',b2.shape)\n",
    "    print('w_out: ',w_out.shape)\n",
    "    print('fc1: ', fc1.shape)\n",
    "    print('activation_1: ', activation_1.shape)\n",
    "    print('fc2: ', fc2.shape)\n",
    "    print('activation_2: ',activation_2.shape)\n",
    "    print('output: ',output.shape)\n",
    "    print('class_probs: ', class_probs.shape)\n",
    "    '''\n",
    "    return w1,b1,w2,b2,w_out\n",
    "    \n",
    "\n",
    "w1,b1,w2,b2,w_out = backward(w1,w2,b1,b2,w_out, b_out,fc1, activation_1, fc2, activation_2,output,class_probs, cost,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_2.dot(w_out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = (np.random.randn(4,10)-1)*.0001\n",
    "b1 = (np.random.randn(10)-1)*.0001\n",
    "w2 = (np.random.randn(10,10)-1)*.0001\n",
    "b2 = (np.random.randn(10)-1)*.0001\n",
    "w_out = (np.random.randn(10,3)-1)*.0001\n",
    "b_out = (np.random.randn(3)-1)*.0001\n",
    "y = y_train\n",
    "learning_rate = 0.001\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000118680126322\n",
      "-0.000236250193984\n"
     ]
    }
   ],
   "source": [
    "print(w_out.max())\n",
    "print(b1.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.77840876758e-05\n",
      "-2.77842900447e-05\n",
      "-2.77851759904e-05\n",
      "-2.77873758175e-05\n",
      "-2.77909457778e-05\n",
      "-2.77934902287e-05\n",
      "-2.778617777e-05\n",
      "-2.77463517456e-05\n",
      "-2.76249988524e-05\n",
      "-2.73267970774e-05\n",
      "-2.66797793668e-05\n",
      "-2.53907754596e-05\n",
      "-2.29816710655e-05\n",
      "-1.87000748075e-05\n",
      "-1.1396114728e-05\n",
      "6.45313890131e-07\n",
      "1.99305797259e-05\n",
      "5.0059165031e-05\n",
      "9.61234682247e-05\n",
      "0.00016523389597\n",
      "0.000267201722702\n",
      "0.000415415554543\n",
      "0.000627944914881\n",
      "0.000928888257383\n",
      "0.00134993614293\n",
      "0.00193201191814\n",
      "0.00272662539237\n",
      "0.00379613742319\n",
      "0.00521136236607\n",
      "0.00704375450588\n",
      "0.00934809115804\n",
      "0.0121314047618\n",
      "0.0153084804738\n",
      "0.0186590497761\n",
      "0.0218267268431\n",
      "0.0244101351335\n",
      "0.0261426733914\n",
      "0.0270420253679\n",
      "0.0273754447956\n",
      "0.0274543340106\n",
      "0.0274644793061\n",
      "0.0274650428677\n",
      "0.0274650525683\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n",
      "0.0274650525999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    costs = []\n",
    "    for i,x in enumerate(X):\n",
    "        #print(i)\n",
    "        x = x.reshape(1,4)\n",
    "        fc1, activation_1, fc2, activation_2, output, class_probs, cost = forward(x,y[i],w1,b1,w2,b2,w_out,b_out)\n",
    "        w1,b1,w2,b2,w_out = backward(w1,w2,b1,b2,w_out,b_out,fc1, activation_1, fc2, activation_2,output,class_probs, cost,learning_rate)\n",
    "        costs.append(cost)\n",
    "        #print('Cost: ',cost)\n",
    "        #print(b1[0][1])\n",
    "        #print('activation_1: ',activation_1)\n",
    "        #print('activation_2: ',activation_2)\n",
    "        #print('Output: ',output)\n",
    "    #print(b1[0][4])\n",
    "    print(w1[0][2])\n",
    "    #print('Output: ',output)\n",
    "    #print('Class Probs: ',class_probs)\n",
    "    #print('Ave Cost: ',np.mean(costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc1, activation_1, fc2, activation_2, output, class_probs, cost = forward(X[1],y[1],w1,b1,w2,b2,w_out,b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00975022],\n",
       "       [ 0.45229313],\n",
       "       [ 0.53795665]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-23-ab000310a780>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-ab000310a780>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    self.graph = graph\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Come back to this\n",
    "class MLP():\n",
    "    \n",
    "    def __init__(self, graph):\n",
    "    self.graph = graph\n",
    "        \n",
    "class Graph():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def add_FCL(n):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
